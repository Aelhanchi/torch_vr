{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='./data', train=True, download=False, \n",
    "                                   transform=torchvision.transforms.ToTensor)\n",
    "X = mnist.data.flatten(1)/255.0\n",
    "y = mnist.targets\n",
    "N = X.shape[0]\n",
    "d_in = 28*28\n",
    "d_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the model\n",
    "torch.manual_seed(0)\n",
    "class Softmax_regression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(d_in, d_out, bias=True)\n",
    "        \n",
    "        self.layers = [self.lin]\n",
    "\n",
    "        self.input_1 = None\n",
    "        self.output_1 = None\n",
    "        self.input_2 = None\n",
    "\n",
    "        self.inputs = list()\n",
    "        self.outputs_grad = list()\n",
    "\n",
    "    def hook(self, grad):\n",
    "        self.outputs_grad += [grad]\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.outputs_grad.clear()\n",
    "        self.inputs.clear()\n",
    "\n",
    "        self.input_1 = X\n",
    "        self.inputs.append(self.input_1)\n",
    "\n",
    "        self.output_1 = self.lin(self.input_1)\n",
    "        if self.output_1.requires_grad:\n",
    "            self.output_1.register_hook(self.hook)\n",
    "\n",
    "        return torch.nn.functional.log_softmax(\n",
    "            torch.cat([self.output_1, torch.ones_like(self.output_1[:,0:1])], dim=1), dim=1)\n",
    "    \n",
    "# defines the loss and initializes the model\n",
    "loss_func = torch.nn.NLLLoss(reduction='sum')\n",
    "nlps_func = loss_func\n",
    "model = Softmax_regression()\n",
    "\n",
    "# copies the starting point\n",
    "start = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "step_size = 1e-05\n",
    "batch_size = 128\n",
    "\n",
    "# runs the non-accelerate algorithms\n",
    "losses_SGD = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=False, var_reduce=None)\n",
    "#losses_SAG = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=False, var_reduce='SAG')\n",
    "losses_SAGA = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=False, var_reduce='SAGA')\n",
    "\n",
    "# runs the accelerated algorithms\n",
    "losses_acc_SGD = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=True, var_reduce=None)\n",
    "#losses_acc_SAG = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=True, var_reduce='SAG')\n",
    "losses_acc_SAGA = helpers.optimize(X, y, model, loss_func, start, step_size, batch_size, accelerated=True, var_reduce='SAGA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the progress of each of SGD, SAGA, and SAG\n",
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(6.6*2,4))\n",
    "\n",
    "axes[0].plot(losses_SGD[50:], label='SGD')\n",
    "#axes[0].plot(losses_SAG, label='SAG')\n",
    "axes[0].plot(losses_SAGA[50:], label='SAGA')\n",
    "axes[0].legend(loc=\"upper right\");\n",
    "\n",
    "axes[1].plot(losses_acc_SGD[50:], label='acc_SGD')\n",
    "#axes[1].plot(losses_acc_SAG, label='acc_SAG')\n",
    "axes[1].plot(losses_acc_SAGA[50:], label='acc_SAGA')\n",
    "axes[1].legend(loc=\"upper right\");\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "step_size = 1e-05\n",
    "batch_size = 128\n",
    "\n",
    "# runs the non-accelerate algorithms\n",
    "nlps_SGD = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=False, var_reduce=None)\n",
    "#nlps_SAG = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=False, var_reduce='SAG')\n",
    "nlps_SAGA = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=False, var_reduce='SAGA')\n",
    "\n",
    "# runs the accelerated algorithms\n",
    "nlps_acc_SGD = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=True, var_reduce=None)\n",
    "#nlps_acc_SAG = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=True, var_reduce='SAG')\n",
    "nlps_acc_SAGA = helpers.sample(X, y, model, nlp_func, start, step_size, batch_size, accelerated=True, var_reduce='SAGA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the progress of each of SGD, SAGA, and SAG\n",
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(6.6*2,4))\n",
    "\n",
    "axes[0].plot(nlps_SGD[50:], label='SGD')\n",
    "#axes[0].plot(nlps_SAG, label='SAG')\n",
    "axes[0].plot(nlps_SAGA[50:], label='SAGA')\n",
    "axes[0].legend(loc=\"upper right\");\n",
    "\n",
    "axes[1].plot(nlps_acc_SGD[50:], label='acc_SGD')\n",
    "#axes[1].plot(nlps_acc_SAG, label='acc_SAG')\n",
    "axes[1].plot(nlps_acc_SAGA[50:], label='acc_SAGA')\n",
    "axes[1].legend(loc=\"upper right\");\n",
    "plt.legend(loc=\"upper right\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
