{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "os.chdir('..')\n",
    "from torch_vr.prev_grads_layers import PrevGradConv2dLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of test\n",
    "N = 1000\n",
    "\n",
    "c_in = 3\n",
    "h_in = 16\n",
    "w_in = 12\n",
    "c_out = 5\n",
    "kernel_size = (3,3)\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "\n",
    "h_out = math.floor((h_in + 2 * padding - dilation * (kernel_size[0] - 1) - 1)/stride + 1)\n",
    "w_out = math.floor((w_in + 2 * padding - dilation * (kernel_size[1] - 1) - 1)/stride + 1)\n",
    "d_in = h_out * w_out * c_out\n",
    "d_out = 10\n",
    "\n",
    "batch_size = 74\n",
    "iterations = 10\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "threshold = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates fake data\n",
    "X = torch.randn(N, c_in, h_in, w_in)\n",
    "y = torch.multinomial(torch.ones(d_out), N, replacement=True)\n",
    "\n",
    "# defines the model\n",
    "class Conv2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d = torch.nn.Conv2d(c_in, c_out, kernel_size, stride, padding, dilation, bias=True)\n",
    "        self.lin = torch.nn.Linear(d_in, d_out, bias=True)\n",
    "        \n",
    "        self.A0 = None\n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "        self.Z2 = None\n",
    "        self.A2 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A0 = X\n",
    "        self.Z1 = self.conv2d(self.A0)\n",
    "        self.Z1.retain_grad()\n",
    "        self.A1 = torch.nn.functional.relu(self.Z1).flatten(1, -1)\n",
    "        self.Z2 = self.lin(self.A1)\n",
    "        self.A2 = torch.nn.functional.log_softmax(self.Z2, dim=1)\n",
    "        return self.A2\n",
    "    \n",
    "# defines the loss and initializes the model\n",
    "loss_func = torch.nn.NLLLoss(reduction='sum')\n",
    "model = Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes PrevGradCon2dLayer object\n",
    "prev_grads = PrevGradConv2dLayer(N, model.conv2d, rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded.\n"
     ]
    }
   ],
   "source": [
    "# tests the batch_gradient functionality\n",
    "for i in range(iterations):\n",
    "    # generates the gradients\n",
    "    model.zero_grad()\n",
    "    indices = torch.multinomial(torch.ones(N), batch_size, replacement=False)\n",
    "    loss = loss_func(model(X[indices]), y[indices])\n",
    "    loss.backward()\n",
    "    \n",
    "    # stores generated gradients\n",
    "    prev_grads.update(model.Z1.grad, model.A0, indices)\n",
    "    \n",
    "    # compares the stored gradients with the actual gradients\n",
    "    dW, db = prev_grads.batch_gradient(indices)\n",
    "    max_error_dW = torch.max(dW - model.conv2d.weight.grad)\n",
    "    max_error_db = torch.max(db - model.conv2d.bias.grad)\n",
    "    if max(max_error_dW, max_error_db) > threshold:\n",
    "        print('Failed.')\n",
    "        break\n",
    "print('Succeeded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded.\n"
     ]
    }
   ],
   "source": [
    "# tests the individual gradients functionality\n",
    "for i in range(iterations):\n",
    "    # generates the gradients\n",
    "    model.zero_grad()\n",
    "    indices = torch.multinomial(torch.ones(N), batch_size, replacement=False)\n",
    "    loss = loss_func(model(X[indices]), y[indices])\n",
    "    loss.backward()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # stores generated gradients\n",
    "    prev_grads.update(model.Z1.grad, model.A0, indices)    \n",
    "    \n",
    "    # generates the individual gradients naively\n",
    "    ind_grads_dW = torch.zeros(batch_size, c_out, c_in, kernel_size[0], kernel_size[1])\n",
    "    ind_grads_db = torch.zeros(batch_size, c_out)\n",
    "    for j in range(batch_size):\n",
    "        loss = loss_func(model(X[indices[j]:(indices[j] + 1)]), y[indices[j]:(indices[j] + 1)])\n",
    "        loss.backward()\n",
    "        ind_grads_dW[j] = model.conv2d.weight.grad\n",
    "        ind_grads_db[j] = model.conv2d.bias.grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "    # compares the stored individual gradients with the actual individual gradients\n",
    "    dW, db = prev_grads.individual_gradients(indices)\n",
    "    max_error_dW = torch.max(dW - ind_grads_dW)\n",
    "    max_error_db = torch.max(db - ind_grads_db)\n",
    "    if max(max_error_dW, max_error_db) > threshold:\n",
    "        print('Failed.')\n",
    "        break\n",
    "print('Succeeded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded.\n"
     ]
    }
   ],
   "source": [
    "# tests the weighted version of batch_gradient and individual gradients functionalities.\n",
    "for i in range(iterations):\n",
    "    # generates the weights\n",
    "    weights = 100*torch.randn(batch_size)\n",
    "    \n",
    "    # generates the gradients\n",
    "    model.zero_grad()\n",
    "    indices = torch.multinomial(torch.ones(N), batch_size, replacement=False)\n",
    "    loss = loss_func(model(X[indices]), y[indices])\n",
    "    loss.backward()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # stores generated gradients\n",
    "    prev_grads.update(model.Z1.grad, model.A0, indices)    \n",
    "    \n",
    "    # generates the individual gradients naively\n",
    "    ind_grads_dW = torch.zeros(batch_size, c_out, c_in, kernel_size[0], kernel_size[1])\n",
    "    ind_grads_db = torch.zeros(batch_size, c_out)\n",
    "    for j in range(batch_size):\n",
    "        loss = loss_func(model(X[indices[j]:(indices[j] + 1)]), y[indices[j]:(indices[j] + 1)])\n",
    "        loss.backward()\n",
    "        ind_grads_dW[j] = weights[j] * model.conv2d.weight.grad\n",
    "        ind_grads_db[j] = weights[j] * model.conv2d.bias.grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "    # compares the stored individual gradients with the actual individual gradients\n",
    "    dW, db = prev_grads.individual_gradients(indices, weights)\n",
    "    max_error_dW = torch.max(dW - ind_grads_dW)\n",
    "    max_error_db = torch.max(db - ind_grads_db)\n",
    "    if max(max_error_dW, max_error_db) > threshold:\n",
    "        print('Failed.')\n",
    "        break\n",
    "        \n",
    "    # compares the stored batch gradients with the actual batch gradient\n",
    "    dW, db = prev_grads.batch_gradient(indices, weights)\n",
    "    max_error_dW = torch.max(dW - ind_grads_dW.sum(dim=0))\n",
    "    max_error_db = torch.max(db - ind_grads_db.sum(dim=0))\n",
    "    if max(max_error_dW, max_error_db) > threshold:\n",
    "        print('Failed.')\n",
    "        break   \n",
    "print('Succeeded.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
