{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "import PyTorch_VR.prev_grads as prev_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates fake data\n",
    "X = torch.randn(1000, 100)\n",
    "beta = torch.randn(100)\n",
    "bias = torch.randn(1)\n",
    "y = (torch.sigmoid(X @ beta + bias) > 0.5).float()\n",
    "\n",
    "# defines the model\n",
    "class Logistic_regression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 1, bias=True)\n",
    "        \n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A0 = X\n",
    "        self.Z1 = self.lin(self.A0)\n",
    "        self.Z1.retain_grad()\n",
    "        self.A1 = torch.sigmoid(self.Z1.squeeze(1))\n",
    "        return self.A1\n",
    "    \n",
    "# defines the loss and initializes the model\n",
    "loss_func = torch.nn.BCELoss(reduction='sum')\n",
    "model = Logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the prev_grads object\n",
    "model.zero_grad()\n",
    "loss = loss_func(model(X), y)\n",
    "loss.backward()   \n",
    "prev_grads_linear = prev_grads.layers.PrevGradLinearLayer(model.Z1.grad, model.A0, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks batch_gradient functionality\n",
    "# test 1\n",
    "threshold = 1e-04\n",
    "dW, db = prev_grads_linear.batch_gradient(torch.arange(1000))\n",
    "max_error_dW = torch.max(dW - model.lin.weight.grad)\n",
    "max_error_db = torch.max(db - model.lin.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 1: Success.')\n",
    "else:\n",
    "    print('test 1: Failed.')\n",
    "\n",
    "# checks batch gradient and update functionality    \n",
    "# test 2\n",
    "model.zero_grad()\n",
    "indices = np.random.choice(1000, 74, replace=False)\n",
    "loss = loss_func(model(X[indices]), y[indices])\n",
    "loss.backward()\n",
    "prev_grads_linear.update(model.Z1.grad, model.A0, indices)\n",
    "\n",
    "dW, db = prev_grads_linear.batch_gradient(indices)\n",
    "max_error_dW = torch.max(dW - model.lin.weight.grad)\n",
    "max_error_db = torch.max(db - model.lin.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 2: Success.')\n",
    "else:\n",
    "    print('test 2: Failed.')\n",
    "    \n",
    "# checks individual gradients functionality\n",
    "# test 3\n",
    "model.zero_grad()\n",
    "ind_grads_dW = torch.zeros(len(indices), 1, 100)\n",
    "ind_grads_db = torch.zeros(len(indices), 1)\n",
    "for (i, j) in enumerate(indices):\n",
    "    loss = loss_func(model(X[j:(j+1)]), y[j:(j+1)])\n",
    "    loss.backward()\n",
    "    ind_grads_dW[i, 0] = model.lin.weight.grad\n",
    "    ind_grads_db[i] = model.lin.bias.grad\n",
    "    model.zero_grad()\n",
    "\n",
    "dW, db = prev_grads_linear.individual_gradients(indices)\n",
    "max_error_dW = torch.max(dW - ind_grads_dW)\n",
    "max_error_db = torch.max(db - ind_grads_db)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 3: Success.')\n",
    "else:\n",
    "    print('test 3: Failed.')\n",
    "    \n",
    "# checks weighted batch gradient and individual gradients functionality\n",
    "# test 4\n",
    "model.zero_grad()\n",
    "weights = torch.randn(74)\n",
    "weighted_batch_gradient_dW = 0\n",
    "weighted_batch_gradient_db = 0\n",
    "weighted_ind_grads_dW = ind_grads_dW\n",
    "weighted_ind_grads_db = ind_grads_db\n",
    "for i in range(74):\n",
    "    weighted_batch_gradient_dW += weights[i] * ind_grads_dW[i]\n",
    "    weighted_batch_gradient_db += weights[i] * ind_grads_db[i]\n",
    "    weighted_ind_grads_dW[i] = weights[i] * weighted_ind_grads_dW[i]\n",
    "    weighted_ind_grads_db[i] = weights[i] * weighted_ind_grads_db[i]\n",
    "\n",
    "dW, db = prev_grads_linear.batch_gradient(indices, weights)\n",
    "dWs, dbs = prev_grads_linear.individual_gradients(indices, weights)\n",
    "max_error_dW = torch.max(dW - weighted_batch_gradient_dW)\n",
    "max_error_db = torch.max(db - weighted_batch_gradient_db)\n",
    "max_error_dWs = torch.max(dWs - weighted_ind_grads_dW)\n",
    "max_error_dbs = torch.max(dbs - weighted_ind_grads_db)\n",
    "if max(max_error_dW, max_error_db, max_error_dWs, max_error_dbs) < threshold:\n",
    "    print('test 4: Success.')\n",
    "else:\n",
    "    print('test 4: Failed.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates fake data\n",
    "X = torch.randn(1000, 3, 16, 16)\n",
    "y = (torch.rand(1000) > 0.5).float()\n",
    "\n",
    "# defines the model\n",
    "class Conv2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d = torch.nn.Conv2d(3, 5, (3, 3), bias=True)\n",
    "        self.lin = torch.nn.Linear(980, 1, bias=True)\n",
    "        \n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A0 = X\n",
    "        self.Z1 = self.conv2d(self.A0)\n",
    "        self.Z1.retain_grad()\n",
    "        self.A1 = torch.nn.functional.relu(self.Z1).flatten(1, -1)\n",
    "        self.Z2 = self.lin(self.A1)\n",
    "        self.A2 = torch.sigmoid(self.Z2.squeeze(1))\n",
    "        return self.A2\n",
    "    \n",
    "# defines the loss and initializes the model\n",
    "loss_func = torch.nn.BCELoss(reduction='sum')\n",
    "model = Conv2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the prev_grads object\n",
    "model.zero_grad()\n",
    "loss = loss_func(model(X), y)\n",
    "loss.backward()   \n",
    "prev_grads_conv2d = prev_grads.layers.PrevGradConv2DLayer(1000, (3,3), 5, 3, model.Z1.grad, model.A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks batch_gradient functionality\n",
    "# test 1\n",
    "threshold = 1e-04\n",
    "dW, db = prev_grads_conv2d.batch_gradient(torch.arange(1000))\n",
    "max_error_dW = torch.max(dW - model.conv2d.weight.grad)\n",
    "max_error_db = torch.max(db - model.conv2d.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 1: Success.')\n",
    "else:\n",
    "    print('test 1: Failed.')\n",
    "    \n",
    "# checks batch gradient and update functionality    \n",
    "# test 2\n",
    "model.zero_grad()\n",
    "indices = np.random.choice(1000, 74, replace=False)\n",
    "loss = loss_func(model(X[indices]), y[indices])\n",
    "loss.backward()\n",
    "prev_grads_conv2d.update(model.Z1.grad, model.A0, indices)\n",
    "\n",
    "dW, db = prev_grads_conv2d.batch_gradient(indices)\n",
    "max_error_dW = torch.max(dW - model.conv2d.weight.grad)\n",
    "max_error_db = torch.max(db - model.conv2d.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 2: Success.')\n",
    "else:\n",
    "    print('test 2: Failed.')\n",
    "    \n",
    "# checks individual gradients functionality\n",
    "# test 3\n",
    "model.zero_grad()\n",
    "ind_grads_dW = torch.zeros(len(indices), 5, 3, 3, 3)\n",
    "ind_grads_db = torch.zeros(len(indices), 5)\n",
    "for (i, j) in enumerate(indices):\n",
    "    loss = loss_func(model(X[j:(j+1)]), y[j:(j+1)])\n",
    "    loss.backward()\n",
    "    ind_grads_dW[i] = model.conv2d.weight.grad\n",
    "    ind_grads_db[i] = model.conv2d.bias.grad\n",
    "    model.zero_grad()\n",
    "\n",
    "dW, db = prev_grads_conv2d.individual_gradients(indices)\n",
    "max_error_dW = torch.max(dW - ind_grads_dW)\n",
    "max_error_db = torch.max(db - ind_grads_db)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 3: Success.')\n",
    "else:\n",
    "    print('test 3: Failed.')\n",
    "    \n",
    "# checks weighted batch gradient and individual gradients functionality\n",
    "# test 4\n",
    "model.zero_grad()\n",
    "weights = torch.randn(74)\n",
    "weighted_batch_gradient_dW = 0\n",
    "weighted_batch_gradient_db = 0\n",
    "weighted_ind_grads_dW = ind_grads_dW\n",
    "weighted_ind_grads_db = ind_grads_db\n",
    "for i in range(74):\n",
    "    weighted_batch_gradient_dW += weights[i] * ind_grads_dW[i]\n",
    "    weighted_batch_gradient_db += weights[i] * ind_grads_db[i]\n",
    "    weighted_ind_grads_dW[i] = weights[i] * weighted_ind_grads_dW[i]\n",
    "    weighted_ind_grads_db[i] = weights[i] * weighted_ind_grads_db[i]\n",
    "\n",
    "dW, db = prev_grads_conv2d.batch_gradient(indices, weights)\n",
    "dWs, dbs = prev_grads_conv2d.individual_gradients(indices, weights)\n",
    "max_error_dW = torch.max(dW - weighted_batch_gradient_dW)\n",
    "max_error_db = torch.max(db - weighted_batch_gradient_db)\n",
    "max_error_dWs = torch.max(dWs - weighted_ind_grads_dW)\n",
    "max_error_dbs = torch.max(dbs - weighted_ind_grads_db)\n",
    "if max(max_error_dW, max_error_db, max_error_dWs, max_error_dbs) < threshold:\n",
    "    print('test 4: Success.')\n",
    "else:\n",
    "    print('test 4: Failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the prev_grads object\n",
    "model.zero_grad()\n",
    "loss = loss_func(model(X), y)\n",
    "loss.backward()   \n",
    "prev_grads_conv2d = prev_grads.layers.PrevGradConv2DLayer(1000, (3,3), 5, 3,\n",
    "                                                          model.Z1.grad, model.A0,  rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks batch_gradient functionality\n",
    "# test 1\n",
    "threshold = 1e-04\n",
    "dW, db = prev_grads_conv2d.batch_gradient(torch.arange(1000))\n",
    "max_error_dW = torch.max(dW - model.conv2d.weight.grad)\n",
    "max_error_db = torch.max(db - model.conv2d.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 1: Success.')\n",
    "else:\n",
    "    print('test 1: Failed.')\n",
    "    \n",
    "# checks batch gradient and update functionality    \n",
    "# test 2\n",
    "model.zero_grad()\n",
    "indices = np.random.choice(1000, 74, replace=False)\n",
    "loss = loss_func(model(X[indices]), y[indices])\n",
    "loss.backward()\n",
    "prev_grads_conv2d.update(model.Z1.grad, model.A0, indices)\n",
    "\n",
    "dW, db = prev_grads_conv2d.batch_gradient(indices)\n",
    "max_error_dW = torch.max(dW - model.conv2d.weight.grad)\n",
    "max_error_db = torch.max(db - model.conv2d.bias.grad)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 2: Success.')\n",
    "else:\n",
    "    print('test 2: Failed.')\n",
    "    \n",
    "# checks individual gradients functionality\n",
    "# test 3\n",
    "model.zero_grad()\n",
    "ind_grads_dW = torch.zeros(len(indices), 5, 3, 3, 3)\n",
    "ind_grads_db = torch.zeros(len(indices), 5)\n",
    "for (i, j) in enumerate(indices):\n",
    "    loss = loss_func(model(X[j:(j+1)]), y[j:(j+1)])\n",
    "    loss.backward()\n",
    "    ind_grads_dW[i] = model.conv2d.weight.grad\n",
    "    ind_grads_db[i] = model.conv2d.bias.grad\n",
    "    model.zero_grad()\n",
    "\n",
    "dW, db = prev_grads_conv2d.individual_gradients(indices)\n",
    "max_error_dW = torch.max(dW - ind_grads_dW)\n",
    "max_error_db = torch.max(db - ind_grads_db)\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 3: Success.')\n",
    "else:\n",
    "    print('test 3: Failed.')\n",
    "    \n",
    "# checks weighted batch gradient and individual gradients functionality\n",
    "# test 4\n",
    "model.zero_grad()\n",
    "weights = torch.randn(74)\n",
    "weighted_batch_gradient_dW = 0\n",
    "weighted_batch_gradient_db = 0\n",
    "weighted_ind_grads_dW = ind_grads_dW\n",
    "weighted_ind_grads_db = ind_grads_db\n",
    "for i in range(74):\n",
    "    weighted_batch_gradient_dW += weights[i] * ind_grads_dW[i]\n",
    "    weighted_batch_gradient_db += weights[i] * ind_grads_db[i]\n",
    "    weighted_ind_grads_dW[i] = weights[i] * weighted_ind_grads_dW[i]\n",
    "    weighted_ind_grads_db[i] = weights[i] * weighted_ind_grads_db[i]\n",
    "\n",
    "dW, db = prev_grads_conv2d.batch_gradient(indices, weights)\n",
    "dWs, dbs = prev_grads_conv2d.individual_gradients(indices, weights)\n",
    "max_error_dW = torch.max(dW - weighted_batch_gradient_dW)\n",
    "max_error_db = torch.max(db - weighted_batch_gradient_db)\n",
    "max_error_dWs = torch.max(dWs - weighted_ind_grads_dW)\n",
    "max_error_dbs = torch.max(dbs - weighted_ind_grads_db)\n",
    "if max(max_error_dW, max_error_db, max_error_dWs, max_error_dbs) < threshold:\n",
    "    print('test 4: Success.')\n",
    "else:\n",
    "    print('test 4: Failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates fake data\n",
    "X = torch.randn(1000, 3, 16, 16)\n",
    "y = (torch.rand(1000) > 0.5).float()\n",
    "\n",
    "# defines the model\n",
    "class Conv2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d = torch.nn.Conv2d(3, 5, (3, 3), bias=True)\n",
    "        self.lin = torch.nn.Linear(980, 1, bias=True)\n",
    "        \n",
    "        self.A0 = None\n",
    "        \n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "        \n",
    "        self.Z2 = None\n",
    "        self.A2 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A0 = X\n",
    "        self.Z1 = self.conv2d(self.A0)\n",
    "        self.Z1.retain_grad()\n",
    "        self.A1 = torch.nn.functional.relu(self.Z1).flatten(1, -1)\n",
    "        self.Z2 = self.lin(self.A1)\n",
    "        self.Z2.retain_grad()\n",
    "        self.A2 = torch.sigmoid(self.Z2.squeeze(1))\n",
    "        return self.A2\n",
    "    \n",
    "# defines the loss and initializes the model\n",
    "loss_func = torch.nn.BCELoss(reduction='sum')\n",
    "model = Conv2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the prev_grads object\n",
    "model.zero_grad()\n",
    "loss = loss_func(model(X), y)\n",
    "loss.backward()   \n",
    "layers = [model.conv2d, model.lin]\n",
    "prev_grads_full = prev_grads.prev_grads.PrevGrads(1000, layers, \n",
    "                                        [model.Z1.grad, model.Z2.grad], [model.A0, model.A1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks batch_gradient functionality\n",
    "# test 1\n",
    "threshold = 1e-04\n",
    "grads = prev_grads_full.batch_gradient(torch.arange(1000))\n",
    "max_error_dW = max([torch.max(g[0] - l.weight.grad) for (g, l) in zip(grads, layers)])\n",
    "max_error_db = max([torch.max(g[1] - l.bias.grad) for (g, l) in zip(grads, layers)])\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 1: Success.')\n",
    "else:\n",
    "    print('test 1: Failed.')\n",
    "\n",
    "# checks batch gradient and update functionality    \n",
    "# test 2\n",
    "model.zero_grad()\n",
    "indices = np.random.choice(1000, 74, replace=False)\n",
    "loss = loss_func(model(X[indices]), y[indices])\n",
    "loss.backward()\n",
    "prev_grads_full.update([model.Z1.grad, model.Z2.grad], [model.A0, model.A1], indices)\n",
    "\n",
    "grads = prev_grads_full.batch_gradient(indices)\n",
    "max_error_dW = max([torch.max(g[0] - l.weight.grad) for (g, l) in zip(grads, layers)])\n",
    "max_error_db = max([torch.max(g[1] - l.bias.grad) for (g, l) in zip(grads, layers)])\n",
    "if max(max_error_dW, max_error_db) < threshold:\n",
    "    print('test 2: Success.')\n",
    "else:\n",
    "    print('test 2: Failed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
